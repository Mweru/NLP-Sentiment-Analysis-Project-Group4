{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the BertModel class\n",
    "from classes import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Q9rO7P1S8tyx"
   },
   "outputs": [],
   "source": [
    "# Relevant libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback, get_scheduler\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Binary classification (Positive vs Negative emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 588
    },
    "id": "G8_VI51b-z6s",
    "outputId": "7287a91a-38a6-4f24-aae2-bf51979e9da3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [129/129 08:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.691100</td>\n",
       "      <td>0.696872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.680600</td>\n",
       "      <td>0.678519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.658600</td>\n",
       "      <td>0.616132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (binary classification):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.76      0.74        46\n",
      "           1       0.69      0.64      0.67        39\n",
      "\n",
      "    accuracy                           0.71        85\n",
      "   macro avg       0.70      0.70      0.70        85\n",
      "weighted avg       0.71      0.71      0.70        85\n",
      "\n",
      "Confusion Matrix (binary classification):\n",
      "[[35 11]\n",
      " [14 25]]\n"
     ]
    }
   ],
   "source": [
    "# For binary classification (Positive vs Negative emotion)\n",
    "binary_model = BertModel(data_path=\"df_raw.csv\", model_type=\"distilbert-base-uncased\", mode=\"binary\")\n",
    "\n",
    "# Load and process data\n",
    "binary_model.load_and_process_data()\n",
    "\n",
    "# Tokenize the data\n",
    "binary_model.tokenize_data()\n",
    "\n",
    "# Build the model\n",
    "binary_model.build_model()\n",
    "\n",
    "# Train the model (you can adjust epochs and batch size as needed)\n",
    "binary_model.train_model(epochs=3, batch_size=8)\n",
    "\n",
    "# Evaluate the model\n",
    "binary_model.evaluate_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from the BERT binary classification model suggest a moderate performance in sentiment analysis of emotions in tweets. The training and validation losses decrease over the epochs, indicating that the model is improving its learning. The classification report shows a precision of 0.71 for class 0 (likely representing negative sentiment) and 0.69 for class 1 (positive sentiment). Recall for class 0 is slightly higher (0.76), indicating that the model is better at identifying negative emotions. However, the recall for class 1 is lower (0.64), meaning the model misses more positive sentiment instances. The F1-scores are balanced, with class 0 performing slightly better. The confusion matrix highlights that there are 35 true negatives, 25 true positives, 14 false positives, and 11 false negatives. Overall, the model has a 71% accuracy, which is decent but shows room for improvement in detecting positive emotions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-class classification (Negative, Neutral, Positive emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "id": "kS9Pbkf9-1MQ",
    "outputId": "1c2a8134-dbf9-4a56-9c5b-a65789adeecb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1539' max='1539' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1539/1539 1:34:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.527200</td>\n",
       "      <td>0.455341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.149300</td>\n",
       "      <td>0.381938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.191900</td>\n",
       "      <td>0.420890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (multi classification):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.97       361\n",
      "           1       0.87      0.85      0.86       332\n",
      "           2       0.88      0.84      0.86       332\n",
      "\n",
      "    accuracy                           0.90      1025\n",
      "   macro avg       0.90      0.89      0.89      1025\n",
      "weighted avg       0.90      0.90      0.90      1025\n",
      "\n",
      "Confusion Matrix (multi classification):\n",
      "[[359   0   2]\n",
      " [ 15 281  36]\n",
      " [  9  43 280]]\n"
     ]
    }
   ],
   "source": [
    "# For multi-class classification (Negative, Neutral, Positive emotion)\n",
    "multi_model = BertModel(data_path=\"df_raw.csv\", model_type=\"distilbert-base-uncased\", mode=\"multi\")\n",
    "\n",
    "# Load and process data\n",
    "multi_model.load_and_process_data()\n",
    "\n",
    "# Tokenize the data\n",
    "multi_model.tokenize_data()\n",
    "\n",
    "# Build the model\n",
    "multi_model.build_model()\n",
    "\n",
    "# Train the model (you can adjust epochs and batch size as needed)\n",
    "multi_model.train_model(epochs=3, batch_size=8)\n",
    "\n",
    "# Evaluate the model\n",
    "multi_model.evaluate_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uiOmSSm1RAnF"
   },
   "source": [
    "The multi-class classification model performs well in predicting three distinct classes, achieving an overall accuracy of 90%. Class 0 is the most accurately predicted, with high precision (0.94) and recall (0.99). Class 1 and Class 2 show slightly lower performance, with precision and recall values around 0.87-0.88 and 0.84-0.85, respectively, indicating some misclassification between these two classes. The confusion matrix highlights that most predictions are correct, but there are a few errors, particularly between Class 1 and Class 2. Overall, the model demonstrates strong and balanced performance across all classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of the 2 bert models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Model  Accuracy  Precision  Recall  F1-Score\n",
      "0      BERT Binary      0.71       0.71    0.76      0.74\n",
      "1  BERT Multiclass      0.90       0.90    0.89      0.89\n"
     ]
    }
   ],
   "source": [
    "# Data for the models\n",
    "data = {\n",
    "    'Model': ['BERT Binary', 'BERT Multiclass'],\n",
    "    'Accuracy': [0.71, 0.90],\n",
    "    'Precision': [0.71, 0.90],\n",
    "    'Recall': [0.76, 0.89],\n",
    "    'F1-Score': [0.74, 0.89]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the table\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BERT Multiclass model demonstrates significantly better performance compared to the BERT Binary model, with higher accuracy, precision, recall, and F1-score across all metrics. This suggests that BERT's ability to handle complex, multi-class sentiment classification tasks leads to more accurate and reliable predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (api)",
   "language": "python",
   "name": "api"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
