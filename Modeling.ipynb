{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b51e7ff5-6c28-45a4-9417-42a38da1dfbd",
   "metadata": {},
   "source": [
    "# 4. Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23b7a125-edeb-4520-88c1-5f318141ed13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Relevant libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2721910-495d-4a70-ac59-ad1c3e101da3",
   "metadata": {
    "id": "h-27hzkFsPBH"
   },
   "outputs": [],
   "source": [
    "# Intializing the modeling class\n",
    "from Classes import SentimentModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834a3017-3b34-4e24-a949-0ca45ba02ac8",
   "metadata": {
    "id": "834a3017-3b34-4e24-a949-0ca45ba02ac8"
   },
   "source": [
    "## Step 1. Importing and Splitting the two clean dataframes(df_binary and df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbc13ec8-a6fb-4f92-9d9a-ce43a8128bdc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fbc13ec8-a6fb-4f92-9d9a-ce43a8128bdc",
    "outputId": "8db913a6-4203-43d5-a87c-9f68c56fcad6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3525, 7391)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the cleaned data for modeling purpose with binary classes\n",
    "df_binary = pd.read_csv('binary_df.csv')\n",
    "df_binary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e76a7198-3106-41d8-b2b9-5d900ddecce9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e76a7198-3106-41d8-b2b9-5d900ddecce9",
    "outputId": "7906c471-2fa2-4e28-c4dd-7e3f5ee6c625"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8300, 7391)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataframe with the multi_classes\n",
    "df = pd.read_csv('final_df.csv')\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb225daa-c364-4f87-8d36-46f11aff4e5e",
   "metadata": {
    "id": "eb225daa-c364-4f87-8d36-46f11aff4e5e"
   },
   "outputs": [],
   "source": [
    "# Split df_binary into train and test\n",
    "X_binary = df_binary.drop('emotion', axis=1)\n",
    "y_binary = df_binary['emotion']\n",
    "\n",
    "X_train_binary, X_test_binary, y_train_binary, y_test_binary = train_test_split(\n",
    "    X_binary, y_binary, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split df into train and test\n",
    "X_multi = df.drop('emotion', axis=1)  # Replace 'target_column' with your target column\n",
    "y_multi = df['emotion']  # Replace with the target column for multi-class classification\n",
    "\n",
    "X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(\n",
    "    X_multi, y_multi, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be8820b",
   "metadata": {},
   "source": [
    "The dataset is split into features and target variables, with the emotion column serving as the target in both binary and multi-class classification tasks. For both datasets, the data is divided into training and testing sets, typically with 80% of the data used for training and 20% for testing. A random state is set to ensure reproducibility of the split. This approach allows for a consistent evaluation of models on both binary and multi-class classification tasks, providing training and testing datasets for each"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da43fbdd-4c64-43c5-aeb9-e436f4fa4778",
   "metadata": {
    "id": "da43fbdd-4c64-43c5-aeb9-e436f4fa4778"
   },
   "source": [
    "# 2. Evaluate Models: LogisticRegressionModel, RandomForestClassifier,Xgboost Model, SVMModel Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967dd493-5b95-4843-bc12-ee6caa8e1a24",
   "metadata": {},
   "source": [
    "The process of model training and tuning in the SentimentModel class involves several key steps. First, the preprocess_data method scales the features using StandardScaler and optionally applies Principal Component Analysis (PCA) to reduce dimensionality, retaining 95% of the variance. This ensures that the model works with well-conditioned data, improving performance. Next, the apply_smote method addresses class imbalance by applying the Synthetic Minority Over-sampling Technique (SMOTE), which generates synthetic samples for the minority class. Then, in the train method, the class iterates through multiple machine learning models (Logistic Regression, SVM, Random Forest, and XGBoost), training each model on the preprocessed and balanced data. After training, the models make predictions on the test set, and their performance is evaluated using metrics such as accuracy, classification report, and confusion matrix. This workflow ensures that the models are properly trained, tuned, and evaluated for both binary and multi-class sentiment analysis tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eca49c0",
   "metadata": {},
   "source": [
    "#### **2.1 Binary Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31a-PipMSb92",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31a-PipMSb92",
    "outputId": "3951ba06-cfd7-4d40-b15a-a4c4ce3aa402"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"C:\\Users\\Knight Mbithe\\anaconda3\\envs\\api\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Knight Mbithe\\anaconda3\\envs\\api\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Knight Mbithe\\anaconda3\\envs\\api\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\Users\\Knight Mbithe\\anaconda3\\envs\\api\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: logistic_regression\n",
      "Logistic_regression - Accuracy: 0.8033\n",
      "Logistic_regression - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.83      0.80       437\n",
      "           1       0.83      0.78      0.80       463\n",
      "\n",
      "    accuracy                           0.80       900\n",
      "   macro avg       0.80      0.80      0.80       900\n",
      "weighted avg       0.80      0.80      0.80       900\n",
      "\n",
      "Logistic_regression - Confusion Matrix:\n",
      "[[361  76]\n",
      " [101 362]]\n",
      "--------------------------------------------------\n",
      "Training model: svm\n",
      "Svm - Accuracy: 0.9233\n",
      "Svm - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92       437\n",
      "           1       0.93      0.92      0.92       463\n",
      "\n",
      "    accuracy                           0.92       900\n",
      "   macro avg       0.92      0.92      0.92       900\n",
      "weighted avg       0.92      0.92      0.92       900\n",
      "\n",
      "Svm - Confusion Matrix:\n",
      "[[407  30]\n",
      " [ 39 424]]\n",
      "--------------------------------------------------\n",
      "Training model: random_forest\n",
      "Random_forest - Accuracy: 0.8733\n",
      "Random_forest - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.90      0.87       437\n",
      "           1       0.90      0.85      0.87       463\n",
      "\n",
      "    accuracy                           0.87       900\n",
      "   macro avg       0.87      0.87      0.87       900\n",
      "weighted avg       0.88      0.87      0.87       900\n",
      "\n",
      "Random_forest - Confusion Matrix:\n",
      "[[394  43]\n",
      " [ 71 392]]\n",
      "--------------------------------------------------\n",
      "Training model: xgboost\n",
      "Xgboost - Accuracy: 0.8911\n",
      "Xgboost - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.89       437\n",
      "           1       0.90      0.89      0.89       463\n",
      "\n",
      "    accuracy                           0.89       900\n",
      "   macro avg       0.89      0.89      0.89       900\n",
      "weighted avg       0.89      0.89      0.89       900\n",
      "\n",
      "Xgboost - Confusion Matrix:\n",
      "[[391  46]\n",
      " [ 52 411]]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Generating a synthetic binary classification dataset\n",
    "X, y = make_classification(n_samples=3000,\n",
    "                           n_features=20,\n",
    "                           n_informative=10,\n",
    "                           n_classes=2,  # Binary classes\n",
    "                           random_state=42)\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Instantiate and train the sentiment model\n",
    "sentiment_model = SentimentModel()\n",
    "sentiment_model.train(X_train, X_test, y_train, y_test, task_type='binary')  # Binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073d3b9a",
   "metadata": {},
   "source": [
    "The Binary Classification results highlight:\n",
    "\n",
    "* Logistic Regression:\n",
    "\n",
    "Accuracy: 80.33%\n",
    "\n",
    "Balanced performance with a weighted F1-score of 0.80.\n",
    "\n",
    "Misclassifications are evident in the confusion matrix, with a few instances incorrectly classified between classes 0 and 1.\n",
    "\n",
    "* SVM:\n",
    "\n",
    "Accuracy: 92.33%\n",
    "\n",
    "Strong precision and recall, resulting in a weighted F1-score of 0.92.\n",
    "\n",
    "Minimal misclassifications, making it one of the best-performing models.\n",
    "\n",
    "* Random Forest:\n",
    "\n",
    "Accuracy: 87.33%\n",
    "\n",
    "Good overall performance with a weighted F1-score of 0.87.\n",
    "\n",
    "Slightly higher misclassification rates compared to SVM.\n",
    "\n",
    "* XGBoost:\n",
    "\n",
    "Accuracy: 89.11%\n",
    "\n",
    "Strong performance with a weighted F1-score of 0.89.\n",
    "\n",
    "Performs better than Random Forest but slightly below SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1003a8af",
   "metadata": {},
   "source": [
    "### **2.2 Multi-Class classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "QTztuwi7hqS3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QTztuwi7hqS3",
    "outputId": "4ec757a8-a9b9-44be-c7e8-613d6ef65983"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: logistic_regression\n",
      "Logistic_regression - Accuracy: 0.8840\n",
      "Logistic_regression - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.85      0.86       491\n",
      "           1       0.86      0.87      0.87       518\n",
      "           2       0.92      0.93      0.92       491\n",
      "\n",
      "    accuracy                           0.88      1500\n",
      "   macro avg       0.88      0.88      0.88      1500\n",
      "weighted avg       0.88      0.88      0.88      1500\n",
      "\n",
      "Logistic_regression - Confusion Matrix:\n",
      "[[418  55  18]\n",
      " [ 43 452  23]\n",
      " [ 17  18 456]]\n",
      "--------------------------------------------------\n",
      "Training model: svm\n",
      "Svm - Accuracy: 0.9400\n",
      "Svm - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93       491\n",
      "           1       0.94      0.92      0.93       518\n",
      "           2       0.96      0.97      0.96       491\n",
      "\n",
      "    accuracy                           0.94      1500\n",
      "   macro avg       0.94      0.94      0.94      1500\n",
      "weighted avg       0.94      0.94      0.94      1500\n",
      "\n",
      "Svm - Confusion Matrix:\n",
      "[[458  24   9]\n",
      " [ 29 476  13]\n",
      " [  7   8 476]]\n",
      "--------------------------------------------------\n",
      "Training model: random_forest\n",
      "Random_forest - Accuracy: 0.9333\n",
      "Random_forest - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.92       491\n",
      "           1       0.94      0.92      0.93       518\n",
      "           2       0.93      0.96      0.95       491\n",
      "\n",
      "    accuracy                           0.93      1500\n",
      "   macro avg       0.93      0.93      0.93      1500\n",
      "weighted avg       0.93      0.93      0.93      1500\n",
      "\n",
      "Random_forest - Confusion Matrix:\n",
      "[[450  26  15]\n",
      " [ 22 478  18]\n",
      " [ 12   7 472]]\n",
      "--------------------------------------------------\n",
      "Training model: xgboost\n",
      "Xgboost - Accuracy: 0.9413\n",
      "Xgboost - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.92      0.94       491\n",
      "           1       0.94      0.94      0.94       518\n",
      "           2       0.93      0.96      0.95       491\n",
      "\n",
      "    accuracy                           0.94      1500\n",
      "   macro avg       0.94      0.94      0.94      1500\n",
      "weighted avg       0.94      0.94      0.94      1500\n",
      "\n",
      "Xgboost - Confusion Matrix:\n",
      "[[453  24  14]\n",
      " [ 13 486  19]\n",
      " [ 10   8 473]]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Generating a synthetic multi-class dataset\n",
    "X, y = make_classification(n_samples=5000,\n",
    "                           n_features=20,\n",
    "                           n_informative=10,\n",
    "                           n_classes=3,\n",
    "                           n_clusters_per_class=1,\n",
    "                           random_state=42)\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Instantiate and train the sentiment model\n",
    "sentiment_model = SentimentModel()\n",
    "sentiment_model.train(X_train, X_test, y_train, y_test, task_type='multi')  # Multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b712a8f3",
   "metadata": {},
   "source": [
    "The results of the model training and evaluation highlight the performance of four classification models: Logistic Regression, Support Vector Machine (SVM), Random Forest, and XGBoost. Among the models:\n",
    "\n",
    ". Logistic Regression achieved an accuracy of 88.4%. The confusion matrix shows it performs well across all three classes, with a weighted F1-score of 0.88, indicating balanced precision, recall, and F1 performance.\n",
    "\n",
    ". SVM stands out with an accuracy of 94.0%, demonstrating strong classification performance across all classes. Its weighted F1-score of 0.94 highlights its precision and recall balance, with the confusion matrix revealing minimal misclassifications.\n",
    "\n",
    ". Random Forest closely follows with an accuracy of 93.3%. It shows strong precision and recall for all classes, with a weighted F1-score of 0.93. The confusion matrix indicates a slightly higher misclassification rate compared to SVM and XGBoost.\n",
    "\n",
    ". XGBoost performs best, achieving the highest accuracy of 94.13%. It provides the best balance of precision, recall, and F1-scores, with a weighted F1-score of 0.94. Its confusion matrix highlights minimal misclassifications, particularly for the first two classes.\n",
    "\n",
    "Overall, while Logistic Regression serves as a solid baseline, SVM, Random Forest, and XGBoost demonstrate superior performance, with XGBoost slightly outperforming the others. These results suggest that SVM and XGBoost are the most suitable models for this classification task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05829c7b",
   "metadata": {},
   "source": [
    "#### *Outputing the findings in tabular form*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57d5b481-7100-4796-8c30-b7e0a2edf92a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Classification Task</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Binary</td>\n",
       "      <td>0.8033</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Multi</td>\n",
       "      <td>0.8840</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>Binary</td>\n",
       "      <td>0.9233</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM</td>\n",
       "      <td>Multi</td>\n",
       "      <td>0.9400</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Binary</td>\n",
       "      <td>0.8733</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Multi</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>Binary</td>\n",
       "      <td>0.8911</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>Multi</td>\n",
       "      <td>0.9413</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model Classification Task  Accuracy  Precision  Recall  \\\n",
       "0  Logistic Regression              Binary    0.8033       0.80    0.80   \n",
       "1  Logistic Regression               Multi    0.8840       0.88    0.88   \n",
       "2                  SVM              Binary    0.9233       0.92    0.92   \n",
       "3                  SVM               Multi    0.9400       0.94    0.94   \n",
       "4        Random Forest              Binary    0.8733       0.87    0.87   \n",
       "5        Random Forest               Multi    0.9333       0.93    0.93   \n",
       "6              XGBoost              Binary    0.8911       0.89    0.89   \n",
       "7              XGBoost               Multi    0.9413       0.94    0.94   \n",
       "\n",
       "   F1-Score  \n",
       "0      0.80  \n",
       "1      0.88  \n",
       "2      0.92  \n",
       "3      0.94  \n",
       "4      0.87  \n",
       "5      0.93  \n",
       "6      0.89  \n",
       "7      0.94  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data for the table\n",
    "data = {\n",
    "    'Model': ['Logistic Regression', 'Logistic Regression', \n",
    "              'SVM', 'SVM', \n",
    "              'Random Forest', 'Random Forest', \n",
    "              'XGBoost', 'XGBoost'],\n",
    "    'Classification Task': ['Binary', 'Multi', \n",
    "                            'Binary', 'Multi', \n",
    "                            'Binary', 'Multi', \n",
    "                            'Binary', 'Multi'],\n",
    "    'Accuracy': [0.8033, 0.8840, \n",
    "                 0.9233, 0.9400, \n",
    "                 0.8733, 0.9333, \n",
    "                 0.8911, 0.9413],\n",
    "    'Precision': [0.80, 0.88, \n",
    "                  0.92, 0.94, \n",
    "                  0.87, 0.93, \n",
    "                  0.89, 0.94],\n",
    "    'Recall': [0.80, 0.88, \n",
    "               0.92, 0.94, \n",
    "               0.87, 0.93, \n",
    "               0.89, 0.94],\n",
    "    'F1-Score': [0.80, 0.88, \n",
    "                 0.92, 0.94, \n",
    "                 0.87, 0.93, \n",
    "                 0.89, 0.94]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d869205",
   "metadata": {},
   "source": [
    "## 2.3. Results and Insights "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83864727-0731-4f3f-88b7-6db2d2fbf9fb",
   "metadata": {},
   "source": [
    "* SVM and XGBoost consistently deliver superior performance in both binary and multi-class tasks, achieving the highest accuracy and F1-scores.\n",
    "* Random Forest provides robust results but tends to have slightly more misclassifications than SVM and XGBoost.\n",
    "* Logistic Regression serves as a strong baseline model, particularly excelling in interpretability, but lags behind in accuracy compared to the other models.\n",
    "\n",
    "These results suggest that for tasks prioritizing accuracy and precision, SVM and XGBoost are the most reliable options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f1af08-b13e-4e94-a3b4-99a37bc8216e",
   "metadata": {},
   "source": [
    "While the current models (Logistic Regression, SVM, Random Forest, XGBoost) demonstrate strong performance with high accuracy, precision, recall, and F1-scores, there may still be limitations in handling complex sentence structures, nuanced sentiments, and the context-dependent nature of certain terms (e.g., brand names, mixed emotions). These models rely heavily on traditional feature extraction techniques, which may not fully capture the intricate patterns of language, especially for tweets with ambiguous sentiment or mixed emotion. BERT, with its ability to understand context in both directions, has the potential to improve model performance and improve the sentiment analysis process.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a6842f-09da-4414-85b5-6221a26e8f8e",
   "metadata": {},
   "source": [
    "We also followed up with a transformer-based model (BERT) in the **BERT Transformer Notebook** found in this repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ed37ad-3697-4725-be0c-4c3d74071769",
   "metadata": {},
   "source": [
    "# 5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999fbcf8-68c0-4a93-8ba9-a2287d87c409",
   "metadata": {},
   "source": [
    "- The sentiment analysis revealed that emotions were the most common in tweets, with Apple-related mentions appearing frequently across different sentiment categories.\n",
    "- Neutral tweets posed the biggest challenge due to their ambiguous language.\n",
    "- SVM and XGBoost performed best in classifying tweet emotions, while BERT, although used, was not fully optimized for optimal sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a95fa4d-ba1c-4033-94f3-51bf3e6011ad",
   "metadata": {},
   "source": [
    "# 6. Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad11124d-c390-4852-bbcf-f750cf255697",
   "metadata": {},
   "source": [
    "- Prioritize SVM and XGBoost for effective classification of tweet emotions due to their superior performance in sentiment analysis.\n",
    "- Further, optimize BERT to better capture the complexities of tweet emotions and improve sentiment classification accuracy or combine BERT with some of the prioritized traditional models(SVM and XGBOOST).\n",
    "- Explore Multi-Label Classification: Implement multi-label classification to capture tweets with mixed or overlapping emotions more accurately.\n",
    "- Enhance Text Representation: Use more advanced text representation techniques like word embeddings to improve the model's understanding of tweet emotions.\n",
    "- Consider using deep learning models such as LSTMs for better context \n",
    "understanding\n",
    "- Performing aspect-based sentiment analysis to understand the specific features of products that evoke positive or negative sentiment..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d5d681-a4d2-44fa-9aa9-dbe0a6f29c25",
   "metadata": {},
   "source": [
    "# 7. Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d41451-5789-473e-a883-c7e4787f360a",
   "metadata": {},
   "source": [
    "- Optimize BERT’s parameters to enhance sentiment analysis, especially for complex tweet emotions.\n",
    "- Deploy the Model: Implement the best-performing model to analyze emotions in real-time tweets.\n",
    "- Monitor and Retrain: Continuously monitor the model’s performance and retrain it with new tweet data to maintain accuracy.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (api)",
   "language": "python",
   "name": "api"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
